<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Rich's Home</title>
  
  <meta name="author" content="Ruiqi(Rich) Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/gif" href="images/paimon.gif">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Ruiqi(Rich) Zhang</name>
              </p>
	      <p>
                Hi, my name is Rich. I'm a first-year Ph.D. student of <a href="https://hiperlab.berkeley.edu/">HiPeR Lab</a> @ UC Berkeley under supervision of <a href="https://scholar.google.com/citations?user=yQxs7qUAAAAJ&oi=ao/">Prof. Mark W. Mueller</a>
               from MechE Department and <a href="https://bair.berkeley.edu">Berkeley AI Rearch(BAIR) Lab</a>.</p>
              <p>
		Before this, I got my Bachelor's degree in Automotive Engineering, Tongji University. I'm also a long-term researcher of Tongji <a href="https://ispc-group.github.io/">ISPC Group</a> supervised by Prof. Guang Chen. My research mainly focuses on Robotics, Reinforcement Learning and Computer Vision. 
		In the 2022 summer, I joined the School of Computing of National University of Singapore and cooperated with <a href="https://linsats.github.io/">Prof. Lin Shao</a> and <a href="https://yusufma03.github.io/">Xiao Ma</a>. We developed a gradient-based RL algorithm via differentiable simulator.</a>
              </p>
          
              <p style="text-align:center">
                <a href="mailto:richzhang@berkeley.edu">Email</a> &nbsp/&nbsp
                <a href="data/cv.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?&user=QXZdP3IAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/ruiqizhang99/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/rich.jpg"><img style="width:130%;max-width:130%" alt="profile photo" src="images/rich.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
	    <p>- [Aug. 2023] Begin my new life at UC Berkeley HiPeR Lab, thanks to great Mark Mueller :) </p>
            <p>- [Sep. 2022] A paper about Multi-Agent Navigation has been accepted to <strong>IEEE MFI</strong> <font color="red">(Best student paper)</font>.</p>
            <p>- [Aug. 2022] I'm joining National University of Singapore, cooperating with Prof. Lin Shao</p>
            <p>- [Jul. 2022] A paper about F1Tenth Autonomous Racing has been accepted to <strong>IEEE RA-L</strong> <font color="red">(My first Paper)</font>!</p>
            <p>- [Apr. 2022] I'm joining Carnegie Mellon University Robotics Institute, cooperating with Prof. Ji Zhang</p>
            <p>- [Dec. 2021] A paper about Relative Pose Estimation has been accepted to Frontiers in NeuroRobotics.</p>
            <p>- [Aug. 2020] I'm joining <strong>ISPC-Lab</strong> as a research scientist working on reinforcement learning.</p>
          </td>
        </tr>
      </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
            </td>
          </tr>
          
        </tbody></table>


            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>

            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='data/zhang2022residual/residual.png' width="160">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9834085">
              <papertitle>Residual Policy Learning Facilitates Efficient Model-Free Autonomous Racing</papertitle>
              </a>
              <br>
              <strong>Ruiqi Zhang</strong>,
              <a href="https://scholar.google.com/citations?user=8mO6YIMAAAAJ&hl=zh-CN">Jing Hou</a>,
              <a href="https://ispc-group.github.io/">Guang Chen*</a>,
              <a href="https://scholar.google.com/citations?user=xxUvEtIAAAAJ&hl=zh-CN">Zhijun Li</a>,
              Jianxiao Chen,
              <a href="https://scholar.google.com/citations?user=-CA8QgwAAAAJ&hl=zh-CN">Alois Knoll</a>
              <br>
		<em>IEEE Robotics and Automation Letters</em>, 2022
              <br>
              <a href="https://ieeexplore.ieee.org/document/9834085">pdf</a>
              /
              <a href="data/zhang2022residual/zhang2022residual.bib">bibtex</a>
              <p>We develop an efficient residual policy learning algorithm with modified artificial potential field for autonomous racing, which leverages complementary property of MAPF and model-free DRL. Meanwhile, we validate its robustness, generalization ability, real-time performance and lap time on 5 tracks of F1Tenth competition. Experimental results show our method outperforms the state-of-the-art method Dreamer and reaches the comparable level of professional human players.</p>
            </td>



            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>

            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='data/zhang2022pipo/pipo.png' width="160">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9913862">
              <papertitle>PIPO: Policy Optimization with Permutation-Invariant Constraint for Distributed Multi-Robot Navigation</papertitle>
              </a>
              <br>
              <strong>Ruiqi Zhang</strong>,
              <a href="https://ispc-group.github.io/">Guang Chen*</a>,
              <a href="https://scholar.google.com/citations?user=8mO6YIMAAAAJ&hl=zh-CN">Jing Hou</a>,
              <a href="https://scholar.google.com/citations?user=xxUvEtIAAAAJ&hl=zh-CN">Zhijun Li</a>,
              <a href="https://scholar.google.com/citations?user=-CA8QgwAAAAJ&hl=zh-CN">Alois Knoll</a>
              <br>
		<em>IEEE International Conference on Multisensor Fusion and Integration</em>, 2022 &nbsp <font color="red">(best student paper)</font>
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/9913862">pdf</a>
              /
              <a href="data/zhang2022pipo/pipo.png">bibtex</a>
              <p>We propose a decentralized reinforcement learning method via graph convolutional network. Our method utilizes the permutation-invariant property in multi-agent system to enhance the representation and generalization ability of actor-critic network. Experimental results show our method is much safer than centralized MARL baselines and constrained barrier function-based methods and can be generalized to arbitrary number of agents.</p>
            </td>
		    
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Pre-Prints</heading>
            </td>
          </tr>
          
        </tbody></table>


            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>

            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='data/hou2023spreeze/spreeze.png' width="160">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9834085">
              <papertitle>Spreeze: High-Throughput Parallel Reinforcement Learning Framework</papertitle>
              </a>
              <br>
	      <a href="https://scholar.google.com/citations?user=8mO6YIMAAAAJ&hl=zh-CN">Jing Hou</a>,
              <a href="https://ispc-group.github.io/">Guang Chen*</a>,
	      <strong>Ruiqi Zhang</strong>,
              <a href="https://scholar.google.com/citations?user=xxUvEtIAAAAJ&hl=zh-CN">Zhijun Li</a>,
              <a href="https://scholar.google.com/citations?user=E1GCDXUAAAAJ&hl=zh-CN&oi=ao">Shangding Gu</a>, 
	      <a href="https://www.cae.cn/cae/html/main/colys/26976335.html">Changjun Jiang</a>
              <br>
		<em>Submitted to IEEE Transactions on Parallel and Distributed Systems</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2312.06126.pdf">Arxiv</a>
		    /
		    bibtex
              <p> We propose a high-performance RL frameworks called Spreeze, which asynchronously parallelizes the experience sampling, network update, performance evaluation, and visualization
		operations, and employs multiple efficient data transmission techniques to transfer various types of data between processes. The
		framework can automatically adjust the parallelization hyperparameters based on the computing ability of the hardware device in order
		to perform efficient large-batch updates.</p>
            </td>

		    
        
	 <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Awards & Scholarship</heading>
            <p>- 2023 Shanghai Excellent Graduate</p>
            <p>- 2022 Enterprise Scholarship of Tongji Uni. (sponsored by Weichai Inc.)</p>
	    <p>- 2021 Silver Prize of Formula Student China (2nd Position)</p>
	    <p>- 2019, 2020 Outstanding Student Scholarship of Tongji Uni. </p>
          </td>
        </tr>
      </tbody></table>
		    

	  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Survice</heading>
            <p>- <strong>Conference Reviewer</strong>: ICRA (2023, 2024), IROS (2024)</p>
            <p>- <strong>Journal Reviewer</strong>: IEEE T-RO, IEEE T-ASE, IEEE RA-L, Frontiers in NeuroRobotics, Robotica</p>
          </td>
        </tr>
      </tbody></table>

	  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Course List</heading>
        <p>- <strong>EE/ME231: Experiential Advanced Control</strong> (24Spring, by Prof. Mark Mueller)</p>
	<p>- <strong>EE/ME232: Advanced Control Systems</strong> (23Fall, by Prof. Roberto Horowitz)</p>
	<p>- <strong>EE/ME236: Dynamics and Control of UAV</strong> (with Lab, 23Fall, by Prof. Mark Mueller)</p>
	<p>- <strong>ME292B: AI for Autonomy</strong> (24Spring, by Wei Zhan)</p>
	<p>- <strong>ME292I: Flight Mechanics</strong> (23Fall, by Thomas Lombaerts)</p>
        
          </td>
        </tr>
      </tbody></table>

	<script type='text/javascript' id='mapmyvisitors' src='https://mapmyvisitors.com/map.js?cl=ffffff&w=400&t=tt&d=ocuKrgWRtManHdC7UcFuvhmkdGJ2AYCdQgu-CdoasiM&co=2d78bb'></script>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Stolen from <a href="https://github.com/jonbarron/website">Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
