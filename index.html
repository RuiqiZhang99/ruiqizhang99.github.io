<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Rich's Home</title>
  
  <meta name="author" content="Ruiqi(Rich) Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/gif" href="images/paimon.gif">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Ruiqi(Rich) Zhang</name>
              </p>
	      <p>
                Hi, there! I'm the second-year Ph.D. student of <a href="https://hiperlab.berkeley.edu/">High Performance Robotics(HiPeR) Lab</a> @ UC Berkeley under supervision of <a href="https://scholar.google.com/citations?user=yQxs7qUAAAAJ&oi=ao/">Prof. Mark W. Mueller</a>
               with both MechE Department and <a href="https://bair.berkeley.edu">Berkeley AI Rearch(BAIR) Lab</a>. 
	      </p>
	      <p>
		Before this, I got my Bachelor's degree in Automotive Engineering, Tongji University. I'm also a long-term researcher of Tongji <a href="https://ispc-group.github.io/">ISPC Group</a> supervised by Prof. Guang Chen. My research mainly focuses on Robotics, Reinforcement Learning and Computer Vision. 
		In the 2022 summer, I joined the School of Computing of National University of Singapore and cooperated with <a href="https://linsats.github.io/">Prof. Lin Shao</a> and <a href="https://yusufma03.github.io/">Xiao Ma</a>. We developed a gradient-based RL algorithm via differentiable simulator.
	      </p>
              <p>
		My research interest lies on Reinforcement Learning, Robotics, Control and Multi-Agent System.
	      </p>
          
              <p style="text-align:center">
                <a href="mailto:richzhang@berkeley.edu">Email</a> &nbsp/&nbsp
                <a href="data/cv.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?&user=QXZdP3IAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/ruiqizhang99/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/rich.jpg"><img style="width:70%;max-width:70%" alt="profile photo" src="images/rich.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
            </td>
          </tr>
          
        </tbody></table>


            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>

            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='data/zhang2022residual/residual.png' width="160">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9834085">
              <papertitle>Residual Policy Learning Facilitates Efficient Model-Free Autonomous Racing</papertitle>
              </a>
              <br>
              <strong>Ruiqi Zhang</strong>,
              <a href="https://scholar.google.com/citations?user=8mO6YIMAAAAJ&hl=zh-CN">Jing Hou</a>,
              <a href="https://ispc-group.github.io/">Guang Chen*</a>,
              <a href="https://scholar.google.com/citations?user=xxUvEtIAAAAJ&hl=zh-CN">Zhijun Li</a>,
              Jianxiao Chen,
              <a href="https://scholar.google.com/citations?user=-CA8QgwAAAAJ&hl=zh-CN">Alois Knoll</a>
              <br>
		<em>IEEE Robotics and Automation Letters</em>, 2022
              <br>
              <a href="https://ieeexplore.ieee.org/document/9834085">pdf</a>
              /
              <a href="data/zhang2022residual/zhang2022residual.bib">bibtex</a>
              <p>We develop an efficient residual policy learning algorithm with modified artificial potential field for autonomous racing, which leverages complementary property of MAPF and model-free DRL. Meanwhile, we validate its robustness, generalization ability, real-time performance and lap time on 5 tracks of F1Tenth competition. Experimental results show our method outperforms the state-of-the-art method Dreamer and reaches the comparable level of professional human players.</p>
            </td>



            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>

            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='data/zhang2022pipo/pipo.png' width="160">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9913862">
              <papertitle>PIPO: Policy Optimization with Permutation-Invariant Constraint for Distributed Multi-Robot Navigation</papertitle>
              </a>
              <br>
              <strong>Ruiqi Zhang</strong>,
              <a href="https://ispc-group.github.io/">Guang Chen*</a>,
              <a href="https://scholar.google.com/citations?user=8mO6YIMAAAAJ&hl=zh-CN">Jing Hou</a>,
              <a href="https://scholar.google.com/citations?user=xxUvEtIAAAAJ&hl=zh-CN">Zhijun Li</a>,
              <a href="https://scholar.google.com/citations?user=-CA8QgwAAAAJ&hl=zh-CN">Alois Knoll</a>
              <br>
		<em>IEEE International Conference on Multisensor Fusion and Integration</em>, 2022 &nbsp <font color="red">(Best Student Paper)</font>
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/9913862">pdf</a>
              /
              <a href="data/zhang2022pipo/pipo.png">bibtex</a>
              <p>We propose a decentralized reinforcement learning method via graph convolutional network. Our method utilizes the permutation-invariant property in multi-agent system to enhance the representation and generalization ability of actor-critic network. Experimental results show our method is much safer than centralized MARL baselines and constrained barrier function-based methods and can be generalized to arbitrary number of agents.</p>
            </td>
		    
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Pre-Prints</heading>
            </td>
          </tr>
	</tbody></table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='data/ruiqi2024survey/survey.png' width="160">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <papertitle>Multi-Agent Reinforcement Learning for Autonomous Driving: A Survey</papertitle>
              <br>
	      <strong>Ruiqi Zhang</strong>,
		      <a href="https://scholar.google.com/citations?user=8mO6YIMAAAAJ">Jing Hou</a>,
		      <a href="https://scholar.google.com/citations?user=Z3QCbaUAAAAJ">Florian Walter</a>, 
		      <a href="https://scholar.google.com/citations?user=E1GCDXUAAAAJ">Shangding Gu</a>, 
		      <a href="https://scholar.google.com/citations?user=PbNNo9cAAAAJ">Jiayi Guan</a>, 
		      <a href="https://scholar.google.com/citations?user=IEOJBbAAAAAJ">Florian RÃ¶hrbein</a>, 
	              <a href="https://ispc-group.github.io/">Guang Chen*</a>,
		      <a href="https://scholar.google.com/citations?user=-CA8QgwAAAAJ&hl=zh-CN">Alois Knoll</a>
              <br>
		<em>Submitted to IEEE Transactions on Robotics</em>, 2024
              <br>
	      <a href="https://github.com/ispc-lab/MARL4AD">Github</a> / Arxiv / bibtex
              <p> We first introduce the existing benchmarks, including advanced simulator, dataset, and competitions for large-scale autonomous driving.
	      Then, we recall the basic concepts in RL and MARL and dive deep into recent SoTA MARL-based methodologies for autonomous driving.
	      We also summarize the tricky challenges in this field and propose promising future directions.</p>
            </td>
        </tbody></table>

	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='data/guan2024cdcp/cdcp.png' width="160">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <papertitle>CDCP: Conditional Diffusion Model with Contextual Prompts for Multi-task Offline Safe  Reinforcement Learning</papertitle>
              <br>
		      <a href="https://scholar.google.com/citations?user=PbNNo9cAAAAJ">Jiayi Guan</a>,
		      <a href="https://scholar.google.com/citations?user=2MXFRAMAAAAJ">Tianle Zhang</a>,
		      <a href="https://scholar.google.com/citations?user=yVhgENIAAAAJ">Li Shen</a>,
	      	      <strong>Ruiqi Zhang</strong>,
		      Lusong Li,
	              <a href="https://ispc-group.github.io/">Guang Chen*</a>,
		      <a href="https://www.cae.cn/cae/html/main/colys/26976335.html">Changjun Jiang</a>
              <br>
		<em>Submitted to IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 2024
              <br>
              Arxiv / bibtex
              <p> In this work, we propose a Conditional Dffusion model with Contextual Prompts (CDCP) to address the multiple challenges encountered in multi-task offline safe RL. 
		We establish the objectives of a multi-task offline safe RL algorithm to address the demands of the multi-task domain for the first time. 
		Subsequently, we convert the multi-task offline-constrained optimization problem into a conditional generation strategy optimization problem using a conditional diffusion model. 
		This method improves the accuracy of multi-task representation by integrating task textual descriptions with trajectory prompts, enhancing the algorithm's adaptability to unseen tasks. 
		Extensive experiments demonstrate that the CDCP algorithm exhibits higher performance and safety in multi-task scenarios than the current state-of-the-art baseline methods.</p>
            </td>
        </tbody></table>


		    
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='data/hou2023spreeze/spreeze.png' width="160">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9834085">
              <papertitle>Spreeze: High-Throughput Parallel Reinforcement Learning Framework</papertitle>
              </a>
              <br>
	      <a href="https://scholar.google.com/citations?user=8mO6YIMAAAAJ&hl=zh-CN">Jing Hou</a>,
              <a href="https://ispc-group.github.io/">Guang Chen*</a>,
	      <strong>Ruiqi Zhang</strong>,
              <a href="https://scholar.google.com/citations?user=xxUvEtIAAAAJ&hl=zh-CN">Zhijun Li</a>,
              <a href="https://scholar.google.com/citations?user=E1GCDXUAAAAJ&hl=zh-CN&oi=ao">Shangding Gu</a>, 
	      <a href="https://www.cae.cn/cae/html/main/colys/26976335.html">Changjun Jiang</a>
              <br>
		<em>Submitted to IEEE Transactions on Parallel and Distributed Systems</em>, 2024
              <br>
              <a href="https://arxiv.org/pdf/2312.06126.pdf">Arxiv</a>
		    /
		    bibtex
              <p> We propose a high-performance RL frameworks called Spreeze, which asynchronously parallelizes the experience sampling, network update, performance evaluation, and visualization
		operations, and employs multiple efficient data transmission techniques to transfer various types of data between processes. The
		framework can automatically adjust the parallelization hyperparameters based on the computing ability of the hardware device in order
		to perform efficient large-batch updates.</p>
            </td>

		    
        
	 <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Awards & Scholarship</heading>
            <p>- 2023 Shanghai Excellent Graduate</p>
            <p>- 2022 Enterprise Scholarship of Tongji Uni. (sponsored by Weichai Inc.)</p>
	    <p>- 2021 Silver Prize of Formula Student China (2nd Position)</p>
	    <p>- 2019, 2020 Outstanding Student Scholarship of Tongji Uni. </p>
          </td>
        </tr>
      </tbody></table>
		    

	  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Survice</heading>
            <p>- <strong>Conference Reviewer</strong>: ICRA (2023, 2024), IROS (2024)</p>
            <p>- <strong>Journal Reviewer</strong>: IEEE T-RO, IEEE T-ASE, IEEE RA-L, Frontiers in NeuroRobotics, Robotica</p>
          </td>
        </tr>
      </tbody></table>

	  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Course List</heading>
        <p>- <strong>EE/ME231: Experiential Advanced Control</strong> (24-Spring, by Prof. Mark W. Mueller)</p>
	<p>- <strong>EE/ME232: Advanced Control Systems</strong> (23-Fall, by Prof. Roberto Horowitz)</p>
	<p>- <strong>EE/ME236: Dynamics and Control of UAV</strong> (with Lab session, 23-Fall, by Prof. Mark W. Mueller)</p>
	<p>- <strong>ME292B: AI for Autonomy</strong> (24-Spring, by Wei Zhan)</p>
	<p>- <strong>ME292I: Flight Mechanics</strong> (23-Fall, by Thomas Lombaerts)</p>
        
          </td>
        </tr>
      </tbody></table>

	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Personal</heading>
	    <p>
		    I'm a big fan of Animation and Hoyoverse games. Besides, I like J-POP music, basketball, baseball and archery.
	    </p>
	    <p>
		    Especially, I'm addicted in <a href="https://zh.wikipedia.org/wiki/YOASOBI">YOASOBI</a> (J-POP band from Japan), <a href="https://en.wikipedia.org/wiki/Shohei_Ohtani">Shohei Ohtani</a> from LA Dodgers(MLB), 
		   and <a href="https://en.wikipedia.org/wiki/Zhou_Guanyu">Guanyu ZHOU</a> from Sauber F1 team.
	    </p>
	    <p>
		    <font color="red">Please feel free to contact with me if you are interested in my research or we have common hobbies. </font>
	    </p>
          </td>
        </tr>
      </tbody></table>

	<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=n&d=kLiReG3btstykaUMs6ubNJ4V3Sz4meSFCTw6uJmqIKc&co=2d78ad&ct=ffffff&cmo=3acc3a&cmn=ff5353"></script>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Stolen from <a href="https://github.com/jonbarron/website">Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
